{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "304b65c7-5eb6-400d-88e9-f3ac60a55e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26348fca-ee0d-4fc2-b63d-86cf0adcd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "df = pd.read_csv('df_att.csv')\n",
    "cat_features = list(df.select_dtypes(exclude='number').drop('Heart Disease', axis=1).columns)\n",
    "df=pd.get_dummies(df, columns = cat_features, dtype=float)\n",
    "df['Heart Disease'] = (df['Heart Disease'] == 'Presence').astype(int)\n",
    "X = df.drop(['Heart Disease', 'Unnamed: 0'], axis=1)\n",
    "y = df['Heart Disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42, shuffle=True)\n",
    "\n",
    "oof_mlp = np.zeros(len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ad29e-6d9d-4b2c-b702-699e38040ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9afd763-e2fd-4559-bd2f-c2867dd508d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.28026396\n",
      "Iteration 2, loss = 0.27523967\n",
      "Iteration 3, loss = 0.27466891\n",
      "Iteration 4, loss = 0.27434826\n",
      "Iteration 5, loss = 0.27409027\n",
      "Iteration 6, loss = 0.27384975\n",
      "Iteration 7, loss = 0.27365203\n",
      "Iteration 8, loss = 0.27349788\n",
      "Iteration 9, loss = 0.27329599\n",
      "Iteration 10, loss = 0.27317072\n",
      "Iteration 11, loss = 0.27311539\n",
      "Iteration 12, loss = 0.27289019\n",
      "Iteration 13, loss = 0.27290262\n",
      "Iteration 14, loss = 0.27263106\n",
      "Iteration 15, loss = 0.27256697\n",
      "Iteration 16, loss = 0.27242130\n",
      "Iteration 17, loss = 0.27239868\n",
      "Iteration 18, loss = 0.27223675\n",
      "Iteration 19, loss = 0.27216644\n",
      "Iteration 20, loss = 0.27216008\n",
      "Iteration 21, loss = 0.27209789\n",
      "Iteration 22, loss = 0.27195363\n",
      "Iteration 23, loss = 0.27195538\n",
      "Iteration 24, loss = 0.27185578\n",
      "Iteration 25, loss = 0.27166822\n",
      "Iteration 26, loss = 0.27173013\n",
      "Iteration 27, loss = 0.27165044\n",
      "Iteration 28, loss = 0.27157187\n",
      "Iteration 29, loss = 0.27160525\n",
      "Iteration 30, loss = 0.27153288\n",
      "Iteration 31, loss = 0.27148921\n",
      "Iteration 32, loss = 0.27143858\n",
      "Iteration 33, loss = 0.27135601\n",
      "Iteration 34, loss = 0.27127454\n",
      "Iteration 35, loss = 0.27130980\n",
      "Iteration 36, loss = 0.27122966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28027934\n",
      "Iteration 2, loss = 0.27511225\n",
      "Iteration 3, loss = 0.27462768\n",
      "Iteration 4, loss = 0.27426418\n",
      "Iteration 5, loss = 0.27389671\n",
      "Iteration 6, loss = 0.27374048\n",
      "Iteration 7, loss = 0.27351530\n",
      "Iteration 8, loss = 0.27329444\n",
      "Iteration 9, loss = 0.27316815\n",
      "Iteration 10, loss = 0.27294905\n",
      "Iteration 11, loss = 0.27291048\n",
      "Iteration 12, loss = 0.27269485\n",
      "Iteration 13, loss = 0.27267609\n",
      "Iteration 14, loss = 0.27246586\n",
      "Iteration 15, loss = 0.27232219\n",
      "Iteration 16, loss = 0.27237461\n",
      "Iteration 17, loss = 0.27218116\n",
      "Iteration 18, loss = 0.27218039\n",
      "Iteration 19, loss = 0.27206390\n",
      "Iteration 20, loss = 0.27199687\n",
      "Iteration 21, loss = 0.27186130\n",
      "Iteration 22, loss = 0.27184345\n",
      "Iteration 23, loss = 0.27179161\n",
      "Iteration 24, loss = 0.27165128\n",
      "Iteration 25, loss = 0.27159173\n",
      "Iteration 26, loss = 0.27151749\n",
      "Iteration 27, loss = 0.27152561\n",
      "Iteration 28, loss = 0.27142241\n",
      "Iteration 29, loss = 0.27135665\n",
      "Iteration 30, loss = 0.27132634\n",
      "Iteration 31, loss = 0.27122332\n",
      "Iteration 32, loss = 0.27123397\n",
      "Iteration 33, loss = 0.27118228\n",
      "Iteration 34, loss = 0.27110669\n",
      "Iteration 35, loss = 0.27110608\n",
      "Iteration 36, loss = 0.27099975\n",
      "Iteration 37, loss = 0.27090790\n",
      "Iteration 38, loss = 0.27084753\n",
      "Iteration 39, loss = 0.27084490\n",
      "Iteration 40, loss = 0.27080421\n",
      "Iteration 41, loss = 0.27069569\n",
      "Iteration 42, loss = 0.27073955\n",
      "Iteration 43, loss = 0.27069963\n",
      "Iteration 44, loss = 0.27067533\n",
      "Iteration 45, loss = 0.27059201\n",
      "Iteration 46, loss = 0.27056065\n",
      "Iteration 47, loss = 0.27045029\n",
      "Iteration 48, loss = 0.27039450\n",
      "Iteration 49, loss = 0.27042743\n",
      "Iteration 50, loss = 0.27035510\n",
      "Iteration 51, loss = 0.27034701\n",
      "Iteration 52, loss = 0.27030140\n",
      "Iteration 53, loss = 0.27025884\n",
      "Iteration 54, loss = 0.27023070\n",
      "Iteration 55, loss = 0.27019525\n",
      "Iteration 56, loss = 0.27013662\n",
      "Iteration 57, loss = 0.27015560\n",
      "Iteration 58, loss = 0.27003940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28099027\n",
      "Iteration 2, loss = 0.27581524\n",
      "Iteration 3, loss = 0.27542610\n",
      "Iteration 4, loss = 0.27513189\n",
      "Iteration 5, loss = 0.27484531\n",
      "Iteration 6, loss = 0.27470731\n",
      "Iteration 7, loss = 0.27439422\n",
      "Iteration 8, loss = 0.27417441\n",
      "Iteration 9, loss = 0.27400484\n",
      "Iteration 10, loss = 0.27396651\n",
      "Iteration 11, loss = 0.27378728\n",
      "Iteration 12, loss = 0.27360635\n",
      "Iteration 13, loss = 0.27348979\n",
      "Iteration 14, loss = 0.27340725\n",
      "Iteration 15, loss = 0.27329377\n",
      "Iteration 16, loss = 0.27319308\n",
      "Iteration 17, loss = 0.27304282\n",
      "Iteration 18, loss = 0.27292707\n",
      "Iteration 19, loss = 0.27284792\n",
      "Iteration 20, loss = 0.27270992\n",
      "Iteration 21, loss = 0.27261110\n",
      "Iteration 22, loss = 0.27257696\n",
      "Iteration 23, loss = 0.27247928\n",
      "Iteration 24, loss = 0.27235779\n",
      "Iteration 25, loss = 0.27231444\n",
      "Iteration 26, loss = 0.27229719\n",
      "Iteration 27, loss = 0.27222423\n",
      "Iteration 28, loss = 0.27216755\n",
      "Iteration 29, loss = 0.27201897\n",
      "Iteration 30, loss = 0.27201491\n",
      "Iteration 31, loss = 0.27187442\n",
      "Iteration 32, loss = 0.27181874\n",
      "Iteration 33, loss = 0.27180500\n",
      "Iteration 34, loss = 0.27172893\n",
      "Iteration 35, loss = 0.27168017\n",
      "Iteration 36, loss = 0.27164957\n",
      "Iteration 37, loss = 0.27156237\n",
      "Iteration 38, loss = 0.27150033\n",
      "Iteration 39, loss = 0.27146431\n",
      "Iteration 40, loss = 0.27140748\n",
      "Iteration 41, loss = 0.27134392\n",
      "Iteration 42, loss = 0.27125793\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28108581\n",
      "Iteration 2, loss = 0.27588486\n",
      "Iteration 3, loss = 0.27546126\n",
      "Iteration 4, loss = 0.27508819\n",
      "Iteration 5, loss = 0.27488726\n",
      "Iteration 6, loss = 0.27465811\n",
      "Iteration 7, loss = 0.27441828\n",
      "Iteration 8, loss = 0.27425251\n",
      "Iteration 9, loss = 0.27404265\n",
      "Iteration 10, loss = 0.27394738\n",
      "Iteration 11, loss = 0.27379939\n",
      "Iteration 12, loss = 0.27359836\n",
      "Iteration 13, loss = 0.27347948\n",
      "Iteration 14, loss = 0.27338883\n",
      "Iteration 15, loss = 0.27320313\n",
      "Iteration 16, loss = 0.27323633\n",
      "Iteration 17, loss = 0.27303684\n",
      "Iteration 18, loss = 0.27297145\n",
      "Iteration 19, loss = 0.27289252\n",
      "Iteration 20, loss = 0.27277589\n",
      "Iteration 21, loss = 0.27274433\n",
      "Iteration 22, loss = 0.27260582\n",
      "Iteration 23, loss = 0.27256156\n",
      "Iteration 24, loss = 0.27250035\n",
      "Iteration 25, loss = 0.27247718\n",
      "Iteration 26, loss = 0.27238854\n",
      "Iteration 27, loss = 0.27227343\n",
      "Iteration 28, loss = 0.27218497\n",
      "Iteration 29, loss = 0.27218600\n",
      "Iteration 30, loss = 0.27210693\n",
      "Iteration 31, loss = 0.27211721\n",
      "Iteration 32, loss = 0.27198638\n",
      "Iteration 33, loss = 0.27188316\n",
      "Iteration 34, loss = 0.27182089\n",
      "Iteration 35, loss = 0.27185992\n",
      "Iteration 36, loss = 0.27171722\n",
      "Iteration 37, loss = 0.27180914\n",
      "Iteration 38, loss = 0.27172795\n",
      "Iteration 39, loss = 0.27158845\n",
      "Iteration 40, loss = 0.27161651\n",
      "Iteration 41, loss = 0.27148798\n",
      "Iteration 42, loss = 0.27145809\n",
      "Iteration 43, loss = 0.27145358\n",
      "Iteration 44, loss = 0.27142698\n",
      "Iteration 45, loss = 0.27134024\n",
      "Iteration 46, loss = 0.27130120\n",
      "Iteration 47, loss = 0.27126438\n",
      "Iteration 48, loss = 0.27129992\n",
      "Iteration 49, loss = 0.27122316\n",
      "Iteration 50, loss = 0.27119609\n",
      "Iteration 51, loss = 0.27120064\n",
      "Iteration 52, loss = 0.27109910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28061736\n",
      "Iteration 2, loss = 0.27561764\n",
      "Iteration 3, loss = 0.27507528\n",
      "Iteration 4, loss = 0.27479673\n",
      "Iteration 5, loss = 0.27454648\n",
      "Iteration 6, loss = 0.27432054\n",
      "Iteration 7, loss = 0.27421583\n",
      "Iteration 8, loss = 0.27398158\n",
      "Iteration 9, loss = 0.27375982\n",
      "Iteration 10, loss = 0.27372210\n",
      "Iteration 11, loss = 0.27351545\n",
      "Iteration 12, loss = 0.27338154\n",
      "Iteration 13, loss = 0.27335285\n",
      "Iteration 14, loss = 0.27326759\n",
      "Iteration 15, loss = 0.27311392\n",
      "Iteration 16, loss = 0.27297683\n",
      "Iteration 17, loss = 0.27291351\n",
      "Iteration 18, loss = 0.27271848\n",
      "Iteration 19, loss = 0.27265525\n",
      "Iteration 20, loss = 0.27244440\n",
      "Iteration 21, loss = 0.27246989\n",
      "Iteration 22, loss = 0.27237195\n",
      "Iteration 23, loss = 0.27224828\n",
      "Iteration 24, loss = 0.27221099\n",
      "Iteration 25, loss = 0.27214123\n",
      "Iteration 26, loss = 0.27204373\n",
      "Iteration 27, loss = 0.27189336\n",
      "Iteration 28, loss = 0.27194904\n",
      "Iteration 29, loss = 0.27185688\n",
      "Iteration 30, loss = 0.27169875\n",
      "Iteration 31, loss = 0.27171402\n",
      "Iteration 32, loss = 0.27165969\n",
      "Iteration 33, loss = 0.27162377\n",
      "Iteration 34, loss = 0.27155665\n",
      "Iteration 35, loss = 0.27142624\n",
      "Iteration 36, loss = 0.27146628\n",
      "Iteration 37, loss = 0.27145519\n",
      "Iteration 38, loss = 0.27132192\n",
      "Iteration 39, loss = 0.27122372\n",
      "Iteration 40, loss = 0.27125233\n",
      "Iteration 41, loss = 0.27117742\n",
      "Iteration 42, loss = 0.27117022\n",
      "Iteration 43, loss = 0.27110421\n",
      "Iteration 44, loss = 0.27102889\n",
      "Iteration 45, loss = 0.27097128\n",
      "Iteration 46, loss = 0.27096138\n",
      "Iteration 47, loss = 0.27083605\n",
      "Iteration 48, loss = 0.27082094\n",
      "Iteration 49, loss = 0.27075461\n",
      "Iteration 50, loss = 0.27085582\n",
      "Iteration 51, loss = 0.27073328\n",
      "Iteration 52, loss = 0.27070597\n",
      "Iteration 53, loss = 0.27068450\n",
      "Iteration 54, loss = 0.27057403\n",
      "Iteration 55, loss = 0.27054307\n",
      "Iteration 56, loss = 0.27055157\n",
      "Iteration 57, loss = 0.27051779\n",
      "Iteration 58, loss = 0.27050913\n",
      "Iteration 59, loss = 0.27044735\n",
      "Iteration 60, loss = 0.27036812\n",
      "Iteration 61, loss = 0.27029136\n",
      "Iteration 62, loss = 0.27035340\n",
      "Iteration 63, loss = 0.27024465\n",
      "Iteration 64, loss = 0.27022796\n",
      "Iteration 65, loss = 0.27015602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr = X_train.iloc[train_idx]\n",
    "    X_val = X_train.iloc[val_idx]\n",
    "    y_tr = y_train.iloc[train_idx]\n",
    "    y_val = y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=(64,32),\n",
    "                         max_iter=500,\n",
    "                         random_state=42,\n",
    "                         verbose=True)\n",
    "    model.fit(X_tr_scaled, y_tr)\n",
    "    oof_mlp[val_idx] = model.predict_proba(X_val_scaled)[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddbcf688-b27c-48a9-8e74-c21a9fb9486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_mlp_csv = pd.DataFrame({'oof_mlp': oof_mlp}).to_csv('oof_mlp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0154636-1f26-4af8-9550-14c0e4081966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
